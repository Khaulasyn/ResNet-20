{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "311c0b53-aee7-481a-8705-8797fd23042e",
   "metadata": {},
   "source": [
    "#EXAMPLE OLD RESNET20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e043b934-e896-4df9-b240-5a3c9af18498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResNet20(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet20, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # First residual block\n",
    "        self.layer1 = self._make_layer(in_channels=32, out_channels=32, num_blocks=3, stride=1)\n",
    "        \n",
    "        # Second residual block\n",
    "        self.layer2 = self._make_layer(in_channels=32, out_channels=64, num_blocks=3, stride=2)\n",
    "        \n",
    "        # Third residual block\n",
    "        self.layer3 = self._make_layer(in_channels=64, out_channels=128, num_blocks=3, stride=2)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc_out = nn.Linear(128, num_classes)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock(in_channels, out_channels, stride))\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels, stride=1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut(x)\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += shortcut\n",
    "        x = self.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eca606-e91a-447f-baec-80c5e7810c01",
   "metadata": {},
   "source": [
    "#EXAMPLE OLD TRAIN AND TEST WITH DATA CIFAR 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a0e03e1-eb7a-45d6-8051-79bf20b8745c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet20():\n",
    "    block = ResidualBlock\n",
    "    model = ResNet20\n",
    "    (5, block)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9818fac2-d13d-483a-9444-2cd5604ecedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Epoch [1/10], Loss: 2.6783, Accuracy: 8.73%\n",
      "Test Loss: 2.6695, Accuracy: 8.80%\n",
      "Epoch [2/10], Loss: 2.6797, Accuracy: 8.75%\n",
      "Test Loss: 2.6706, Accuracy: 8.74%\n",
      "Epoch [3/10], Loss: 2.6825, Accuracy: 8.58%\n",
      "Test Loss: 2.6639, Accuracy: 8.75%\n",
      "Epoch [4/10], Loss: 2.6802, Accuracy: 8.64%\n",
      "Test Loss: 2.6692, Accuracy: 8.80%\n",
      "Epoch [5/10], Loss: 2.6803, Accuracy: 8.62%\n",
      "Test Loss: 2.6691, Accuracy: 8.79%\n",
      "Epoch [6/10], Loss: 2.6804, Accuracy: 8.62%\n",
      "Test Loss: 2.6685, Accuracy: 8.85%\n",
      "Epoch [7/10], Loss: 2.6797, Accuracy: 8.56%\n",
      "Test Loss: 2.6760, Accuracy: 8.74%\n",
      "Epoch [8/10], Loss: 2.6798, Accuracy: 8.82%\n",
      "Test Loss: 2.6690, Accuracy: 8.72%\n",
      "Epoch [9/10], Loss: 2.6806, Accuracy: 8.63%\n",
      "Test Loss: 2.6618, Accuracy: 8.85%\n",
      "Epoch [10/10], Loss: 2.6795, Accuracy: 8.64%\n",
      "Test Loss: 2.6688, Accuracy: 8.72%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from ResNet20 import ResNet20  # assuming ResNet20 is defined in ResNet20.py\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "learning_rate = 0.1\n",
    "\n",
    "# CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model\n",
    "model = ResNet20(num_classes=10).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs.clone())  # Clone inputs to avoid inplace operation\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        #loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss/len(train_loader):.4f}, Accuracy: {100.*correct/total:.2f}%')\n",
    "\n",
    "# Testing loop\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Statistics\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    print(f'Test Loss: {test_loss/len(test_loader):.4f}, Accuracy: {100.*correct/total:.2f}%')\n",
    "\n",
    "# Main training and testing loop\n",
    "for epoch in range(num_epochs):\n",
    "    train(epoch)\n",
    "    test(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fb558bd-4932-4b0f-a264-eccf98b7f526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Epoch [1/100], Loss: 2.6143, Accuracy: 9.42%\n",
      "Test Loss: 2.5721, Accuracy: 9.17%\n",
      "Epoch [2/100], Loss: 2.6132, Accuracy: 9.40%\n",
      "Test Loss: 2.5646, Accuracy: 9.28%\n",
      "Epoch [3/100], Loss: 2.6128, Accuracy: 9.37%\n",
      "Test Loss: 2.5678, Accuracy: 9.19%\n",
      "Epoch [4/100], Loss: 2.6121, Accuracy: 9.46%\n",
      "Test Loss: 2.5623, Accuracy: 9.13%\n",
      "Epoch [5/100], Loss: 2.6128, Accuracy: 9.33%\n",
      "Test Loss: 2.5708, Accuracy: 9.20%\n",
      "Epoch [6/100], Loss: 2.6125, Accuracy: 9.57%\n",
      "Test Loss: 2.5669, Accuracy: 9.13%\n",
      "Epoch [7/100], Loss: 2.6126, Accuracy: 9.53%\n",
      "Test Loss: 2.5639, Accuracy: 9.23%\n",
      "Epoch [8/100], Loss: 2.6179, Accuracy: 9.19%\n",
      "Test Loss: 2.5678, Accuracy: 9.22%\n",
      "Epoch [9/100], Loss: 2.6121, Accuracy: 9.65%\n",
      "Test Loss: 2.5653, Accuracy: 9.25%\n",
      "Epoch [10/100], Loss: 2.6090, Accuracy: 9.50%\n",
      "Test Loss: 2.5666, Accuracy: 9.16%\n",
      "Epoch [11/100], Loss: 2.6123, Accuracy: 9.48%\n",
      "Test Loss: 2.5745, Accuracy: 9.24%\n",
      "Epoch [12/100], Loss: 2.6155, Accuracy: 9.24%\n",
      "Test Loss: 2.5720, Accuracy: 8.97%\n",
      "Epoch [13/100], Loss: 2.6147, Accuracy: 9.52%\n",
      "Test Loss: 2.5798, Accuracy: 9.22%\n",
      "Epoch [14/100], Loss: 2.6144, Accuracy: 9.46%\n",
      "Test Loss: 2.5691, Accuracy: 9.13%\n",
      "Epoch [15/100], Loss: 2.6143, Accuracy: 9.52%\n",
      "Test Loss: 2.5719, Accuracy: 9.06%\n",
      "Epoch [16/100], Loss: 2.6102, Accuracy: 9.51%\n",
      "Test Loss: 2.5720, Accuracy: 9.06%\n",
      "Epoch [17/100], Loss: 2.6147, Accuracy: 9.41%\n",
      "Test Loss: 2.5683, Accuracy: 9.21%\n",
      "Epoch [18/100], Loss: 2.6132, Accuracy: 9.40%\n",
      "Test Loss: 2.5717, Accuracy: 9.21%\n",
      "Epoch [19/100], Loss: 2.6123, Accuracy: 9.34%\n",
      "Test Loss: 2.5651, Accuracy: 9.31%\n",
      "Epoch [20/100], Loss: 2.6150, Accuracy: 9.45%\n",
      "Test Loss: 2.5689, Accuracy: 9.18%\n",
      "Epoch [21/100], Loss: 2.6119, Accuracy: 9.56%\n",
      "Test Loss: 2.5704, Accuracy: 9.17%\n",
      "Epoch [22/100], Loss: 2.6134, Accuracy: 9.46%\n",
      "Test Loss: 2.5721, Accuracy: 9.27%\n",
      "Epoch [23/100], Loss: 2.6116, Accuracy: 9.44%\n",
      "Test Loss: 2.5736, Accuracy: 9.18%\n",
      "Epoch [24/100], Loss: 2.6115, Accuracy: 9.58%\n",
      "Test Loss: 2.5654, Accuracy: 9.18%\n",
      "Epoch [25/100], Loss: 2.6114, Accuracy: 9.49%\n",
      "Test Loss: 2.5703, Accuracy: 9.15%\n",
      "Epoch [26/100], Loss: 2.6146, Accuracy: 9.67%\n",
      "Test Loss: 2.5707, Accuracy: 9.17%\n",
      "Epoch [27/100], Loss: 2.6120, Accuracy: 9.36%\n",
      "Test Loss: 2.5694, Accuracy: 9.17%\n",
      "Epoch [28/100], Loss: 2.6111, Accuracy: 9.56%\n",
      "Test Loss: 2.5702, Accuracy: 9.15%\n",
      "Epoch [29/100], Loss: 2.6103, Accuracy: 9.45%\n",
      "Test Loss: 2.5737, Accuracy: 9.12%\n",
      "Epoch [30/100], Loss: 2.6099, Accuracy: 9.53%\n",
      "Test Loss: 2.5620, Accuracy: 9.48%\n",
      "Epoch [31/100], Loss: 2.6120, Accuracy: 9.75%\n",
      "Test Loss: 2.5690, Accuracy: 9.24%\n",
      "Epoch [32/100], Loss: 2.6142, Accuracy: 9.28%\n",
      "Test Loss: 2.5732, Accuracy: 9.20%\n",
      "Epoch [33/100], Loss: 2.6095, Accuracy: 9.40%\n",
      "Test Loss: 2.5669, Accuracy: 9.25%\n",
      "Epoch [34/100], Loss: 2.6118, Accuracy: 9.25%\n",
      "Test Loss: 2.5723, Accuracy: 9.26%\n",
      "Epoch [35/100], Loss: 2.6118, Accuracy: 9.59%\n",
      "Test Loss: 2.5692, Accuracy: 9.20%\n",
      "Epoch [36/100], Loss: 2.6098, Accuracy: 9.48%\n",
      "Test Loss: 2.5629, Accuracy: 9.13%\n",
      "Epoch [37/100], Loss: 2.6118, Accuracy: 9.56%\n",
      "Test Loss: 2.5673, Accuracy: 9.36%\n",
      "Epoch [38/100], Loss: 2.6132, Accuracy: 9.42%\n",
      "Test Loss: 2.5710, Accuracy: 9.18%\n",
      "Epoch [39/100], Loss: 2.6150, Accuracy: 9.39%\n",
      "Test Loss: 2.5657, Accuracy: 9.22%\n",
      "Epoch [40/100], Loss: 2.6108, Accuracy: 9.47%\n",
      "Test Loss: 2.5682, Accuracy: 9.21%\n",
      "Epoch [41/100], Loss: 2.6132, Accuracy: 9.24%\n",
      "Test Loss: 2.5702, Accuracy: 9.33%\n",
      "Epoch [42/100], Loss: 2.6120, Accuracy: 9.39%\n",
      "Test Loss: 2.5602, Accuracy: 9.17%\n",
      "Epoch [43/100], Loss: 2.6122, Accuracy: 9.44%\n",
      "Test Loss: 2.5671, Accuracy: 9.30%\n",
      "Epoch [44/100], Loss: 2.6118, Accuracy: 9.46%\n",
      "Test Loss: 2.5755, Accuracy: 9.31%\n",
      "Epoch [45/100], Loss: 2.6103, Accuracy: 9.41%\n",
      "Test Loss: 2.5642, Accuracy: 9.18%\n",
      "Epoch [46/100], Loss: 2.6127, Accuracy: 9.36%\n",
      "Test Loss: 2.5685, Accuracy: 9.25%\n",
      "Epoch [47/100], Loss: 2.6169, Accuracy: 9.52%\n",
      "Test Loss: 2.5629, Accuracy: 9.29%\n",
      "Epoch [48/100], Loss: 2.6124, Accuracy: 9.29%\n",
      "Test Loss: 2.5745, Accuracy: 9.28%\n",
      "Epoch [49/100], Loss: 2.6119, Accuracy: 9.56%\n",
      "Test Loss: 2.5678, Accuracy: 9.23%\n",
      "Epoch [50/100], Loss: 2.6153, Accuracy: 9.46%\n",
      "Test Loss: 2.5685, Accuracy: 9.30%\n",
      "Epoch [51/100], Loss: 2.6125, Accuracy: 9.44%\n",
      "Test Loss: 2.5651, Accuracy: 9.13%\n",
      "Epoch [52/100], Loss: 2.6141, Accuracy: 9.33%\n",
      "Test Loss: 2.5646, Accuracy: 9.24%\n",
      "Epoch [53/100], Loss: 2.6143, Accuracy: 9.35%\n",
      "Test Loss: 2.5683, Accuracy: 9.35%\n",
      "Epoch [54/100], Loss: 2.6132, Accuracy: 9.23%\n",
      "Test Loss: 2.5636, Accuracy: 9.20%\n",
      "Epoch [55/100], Loss: 2.6149, Accuracy: 9.40%\n",
      "Test Loss: 2.5728, Accuracy: 9.21%\n",
      "Epoch [56/100], Loss: 2.6115, Accuracy: 9.44%\n",
      "Test Loss: 2.5649, Accuracy: 9.25%\n",
      "Epoch [57/100], Loss: 2.6116, Accuracy: 9.49%\n",
      "Test Loss: 2.5638, Accuracy: 9.18%\n",
      "Epoch [58/100], Loss: 2.6131, Accuracy: 9.51%\n",
      "Test Loss: 2.5716, Accuracy: 9.25%\n",
      "Epoch [59/100], Loss: 2.6153, Accuracy: 9.46%\n",
      "Test Loss: 2.5705, Accuracy: 9.07%\n",
      "Epoch [60/100], Loss: 2.6134, Accuracy: 9.50%\n",
      "Test Loss: 2.5714, Accuracy: 9.28%\n",
      "Epoch [61/100], Loss: 2.6114, Accuracy: 9.45%\n",
      "Test Loss: 2.5682, Accuracy: 9.30%\n",
      "Epoch [62/100], Loss: 2.6108, Accuracy: 9.65%\n",
      "Test Loss: 2.5667, Accuracy: 9.26%\n",
      "Epoch [63/100], Loss: 2.6091, Accuracy: 9.60%\n",
      "Test Loss: 2.5667, Accuracy: 9.19%\n",
      "Epoch [64/100], Loss: 2.6125, Accuracy: 9.41%\n",
      "Test Loss: 2.5691, Accuracy: 9.20%\n",
      "Epoch [65/100], Loss: 2.6115, Accuracy: 9.34%\n",
      "Test Loss: 2.5732, Accuracy: 9.15%\n",
      "Epoch [66/100], Loss: 2.6125, Accuracy: 9.49%\n",
      "Test Loss: 2.5651, Accuracy: 9.17%\n",
      "Epoch [67/100], Loss: 2.6130, Accuracy: 9.40%\n",
      "Test Loss: 2.5779, Accuracy: 9.30%\n",
      "Epoch [68/100], Loss: 2.6122, Accuracy: 9.46%\n",
      "Test Loss: 2.5724, Accuracy: 9.27%\n",
      "Epoch [69/100], Loss: 2.6103, Accuracy: 9.51%\n",
      "Test Loss: 2.5682, Accuracy: 9.12%\n",
      "Epoch [70/100], Loss: 2.6138, Accuracy: 9.48%\n",
      "Test Loss: 2.5660, Accuracy: 9.13%\n",
      "Epoch [71/100], Loss: 2.6131, Accuracy: 9.53%\n",
      "Test Loss: 2.5719, Accuracy: 9.19%\n",
      "Epoch [72/100], Loss: 2.6098, Accuracy: 9.60%\n",
      "Test Loss: 2.5723, Accuracy: 9.14%\n",
      "Epoch [73/100], Loss: 2.6101, Accuracy: 9.37%\n",
      "Test Loss: 2.5767, Accuracy: 9.21%\n",
      "Epoch [74/100], Loss: 2.6102, Accuracy: 9.58%\n",
      "Test Loss: 2.5691, Accuracy: 9.15%\n",
      "Epoch [75/100], Loss: 2.6116, Accuracy: 9.47%\n",
      "Test Loss: 2.5690, Accuracy: 9.27%\n",
      "Epoch [76/100], Loss: 2.6118, Accuracy: 9.49%\n",
      "Test Loss: 2.5739, Accuracy: 9.16%\n",
      "Epoch [77/100], Loss: 2.6100, Accuracy: 9.48%\n",
      "Test Loss: 2.5728, Accuracy: 9.27%\n",
      "Epoch [78/100], Loss: 2.6103, Accuracy: 9.45%\n",
      "Test Loss: 2.5694, Accuracy: 9.29%\n",
      "Epoch [79/100], Loss: 2.6159, Accuracy: 9.40%\n",
      "Test Loss: 2.5673, Accuracy: 9.09%\n",
      "Epoch [80/100], Loss: 2.6128, Accuracy: 9.45%\n",
      "Test Loss: 2.5748, Accuracy: 9.28%\n",
      "Epoch [81/100], Loss: 2.6155, Accuracy: 9.35%\n",
      "Test Loss: 2.5678, Accuracy: 9.14%\n",
      "Epoch [82/100], Loss: 2.6088, Accuracy: 9.67%\n",
      "Test Loss: 2.5659, Accuracy: 9.32%\n",
      "Epoch [83/100], Loss: 2.6110, Accuracy: 9.49%\n",
      "Test Loss: 2.5734, Accuracy: 9.27%\n",
      "Epoch [84/100], Loss: 2.6136, Accuracy: 9.34%\n",
      "Test Loss: 2.5681, Accuracy: 9.17%\n",
      "Epoch [85/100], Loss: 2.6107, Accuracy: 9.44%\n",
      "Test Loss: 2.5768, Accuracy: 9.31%\n",
      "Epoch [86/100], Loss: 2.6134, Accuracy: 9.44%\n",
      "Test Loss: 2.5691, Accuracy: 9.13%\n",
      "Epoch [87/100], Loss: 2.6126, Accuracy: 9.33%\n",
      "Test Loss: 2.5743, Accuracy: 9.21%\n",
      "Epoch [88/100], Loss: 2.6112, Accuracy: 9.49%\n",
      "Test Loss: 2.5809, Accuracy: 9.16%\n",
      "Epoch [89/100], Loss: 2.6085, Accuracy: 9.62%\n",
      "Test Loss: 2.5674, Accuracy: 9.00%\n",
      "Epoch [90/100], Loss: 2.6119, Accuracy: 9.41%\n",
      "Test Loss: 2.5679, Accuracy: 9.08%\n",
      "Epoch [91/100], Loss: 2.6114, Accuracy: 9.41%\n",
      "Test Loss: 2.5781, Accuracy: 9.28%\n",
      "Epoch [92/100], Loss: 2.6150, Accuracy: 9.46%\n",
      "Test Loss: 2.5739, Accuracy: 9.22%\n",
      "Epoch [93/100], Loss: 2.6123, Accuracy: 9.30%\n",
      "Test Loss: 2.5703, Accuracy: 9.16%\n",
      "Epoch [94/100], Loss: 2.6139, Accuracy: 9.46%\n",
      "Test Loss: 2.5732, Accuracy: 9.14%\n",
      "Epoch [95/100], Loss: 2.6112, Accuracy: 9.51%\n",
      "Test Loss: 2.5684, Accuracy: 9.21%\n",
      "Epoch [96/100], Loss: 2.6133, Accuracy: 9.40%\n",
      "Test Loss: 2.5728, Accuracy: 9.20%\n",
      "Epoch [97/100], Loss: 2.6103, Accuracy: 9.49%\n",
      "Test Loss: 2.5629, Accuracy: 9.20%\n",
      "Epoch [98/100], Loss: 2.6117, Accuracy: 9.37%\n",
      "Test Loss: 2.5693, Accuracy: 8.99%\n",
      "Epoch [99/100], Loss: 2.6106, Accuracy: 9.48%\n",
      "Test Loss: 2.5733, Accuracy: 9.19%\n",
      "Epoch [100/100], Loss: 2.6133, Accuracy: 9.45%\n",
      "Test Loss: 2.5683, Accuracy: 9.20%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from ResNet20 import ResNet20  # assuming ResNet20 is defined in ResNet20.py\n",
    "from torchvision import transforms\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "# CIFAR-10 dataset\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform_train, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Model\n",
    "model = ResNet20(num_classes=10).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs.clone())  # Clone inputs to avoid inplace operation\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        #loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss/len(train_loader):.4f}, Accuracy: {100.*correct/total:.2f}%')\n",
    "\n",
    "# Testing loop\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Statistics\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    print(f'Test Loss: {test_loss/len(test_loader):.4f}, Accuracy: {100.*correct/total:.2f}%')\n",
    "\n",
    "# Main training and testing loop\n",
    "for epoch in range(num_epochs):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003817e1-6c8e-49c8-9a78-70ea4dd3261a",
   "metadata": {},
   "source": [
    "OLD MODEL RESNET 20 #EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d0c42c-3c29-4193-823f-ce000f97f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference from kuangliu resnet.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super().__init__()\n",
    "        #self.in_channels = in_channels\n",
    "        #self.out_channels = out_channels\n",
    "        #self.kernel_size = kernel_size\n",
    "        #self.stride = stride\n",
    "        #self.padding = padding\n",
    "        #self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        #self.bn = nn.BatchNorm2d(out_channels) #for stabilize the training process by reducing internal covariate shift\n",
    "        #self.relu = nn.ReLU()\n",
    "\n",
    "#    def forward(self, x):\n",
    "#        x = self.conv(x)\n",
    "#        x = self.bn1(x)\n",
    "#        x = self.relu(x)\n",
    "#        x = self.bn2(x)\n",
    "#        x = self.conv2(x)\n",
    "#        return x\n",
    "#class ConvBlock(nn.Module):\n",
    "#    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "#        super(ConvBlock, self).__init__()\n",
    "#        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "#        self.bn = nn.BatchNorm2d(out_channels)  # Batch normalization layer\n",
    "#        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)  # Apply batch normalization\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample #for dimention matching, ex input 64x64 become 32x32 for clearly image\n",
    "        x += identity #current set feature\n",
    "        x = self.relu(x) #final activation function\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride,first=False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        in_channels//4\n",
    "        self.conv1 = ConvBlock(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.conv2 = ConvBlock(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels,\n",
    "                kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x + residual #because imagine if input + residual = output with high level\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNet20(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet20, self).__init__()\n",
    "        self.conv1 = ConvBlock(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            ResidualBlock(16, 16, stride=1),\n",
    "            ResidualBlock(16, 16, stride=1),\n",
    "            ResidualBlock(16, 16, stride=1)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            ResidualBlock(16, 32, stride=2),\n",
    "            ResidualBlock(32, 32, stride=1),\n",
    "            ResidualBlock(32, 32, stride=1),\n",
    "            ResidualBlock(32, 32, stride=1),\n",
    "            ResidualBlock(32, 32, stride=1)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            ResidualBlock(32, 64, stride=2),\n",
    "            ResidualBlock(64, 64, stride=1),\n",
    "            ResidualBlock(64, 64, stride=1),\n",
    "            ResidualBlock(64, 64, stride=1)\n",
    "        )\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=3)\n",
    "        self.fc = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = ResNet20()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548fe630-a41d-4ba8-9471-556e424448a7",
   "metadata": {},
   "source": [
    "EXAMPLE TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17e148bf-e6c6-47a5-b7f5-bb274aa54216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = F.relu(self.batch_norm1(self.conv1(x)))\n",
    "        out = self.batch_norm2(self.conv2(out))\n",
    "        out += self.shortcut(residual)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batch_norm = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, channels, stride))\n",
    "            self.in_channels = channels * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.batch_norm(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "def ResNet20():\n",
    "    return ResNet(BasicBlock, [3, 3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e33940ab-bdd4-4930-adf1-44972947ab30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Epoch [1/100], Loss: 2.4623, Accuracy: 10.89%\n",
      "Test Loss: 2.4523, Accuracy: 10.31%\n",
      "Epoch [2/100], Loss: 2.4653, Accuracy: 10.78%\n",
      "Test Loss: 2.4529, Accuracy: 10.36%\n",
      "Epoch [3/100], Loss: 2.4613, Accuracy: 10.93%\n",
      "Test Loss: 2.4515, Accuracy: 10.39%\n",
      "Epoch [4/100], Loss: 2.4619, Accuracy: 11.00%\n",
      "Test Loss: 2.4493, Accuracy: 10.33%\n",
      "Epoch [5/100], Loss: 2.4591, Accuracy: 11.01%\n",
      "Test Loss: 2.4527, Accuracy: 10.32%\n",
      "Epoch [6/100], Loss: 2.4639, Accuracy: 10.85%\n",
      "Test Loss: 2.4527, Accuracy: 10.54%\n",
      "Epoch [7/100], Loss: 2.4618, Accuracy: 10.72%\n",
      "Test Loss: 2.4512, Accuracy: 10.21%\n",
      "Epoch [8/100], Loss: 2.4651, Accuracy: 10.63%\n",
      "Test Loss: 2.4536, Accuracy: 10.34%\n",
      "Epoch [9/100], Loss: 2.4622, Accuracy: 10.87%\n",
      "Test Loss: 2.4522, Accuracy: 10.27%\n",
      "Epoch [10/100], Loss: 2.4605, Accuracy: 10.97%\n",
      "Test Loss: 2.4506, Accuracy: 10.39%\n",
      "Epoch [11/100], Loss: 2.4581, Accuracy: 10.64%\n",
      "Test Loss: 2.4502, Accuracy: 10.36%\n",
      "Epoch [12/100], Loss: 2.4611, Accuracy: 10.82%\n",
      "Test Loss: 2.4533, Accuracy: 10.20%\n",
      "Epoch [13/100], Loss: 2.4654, Accuracy: 10.61%\n",
      "Test Loss: 2.4505, Accuracy: 10.36%\n",
      "Epoch [14/100], Loss: 2.4586, Accuracy: 10.89%\n",
      "Test Loss: 2.4521, Accuracy: 10.30%\n",
      "Epoch [15/100], Loss: 2.4624, Accuracy: 10.82%\n",
      "Test Loss: 2.4525, Accuracy: 10.21%\n",
      "Epoch [16/100], Loss: 2.4610, Accuracy: 10.84%\n",
      "Test Loss: 2.4491, Accuracy: 10.39%\n",
      "Epoch [17/100], Loss: 2.4656, Accuracy: 10.75%\n",
      "Test Loss: 2.4516, Accuracy: 10.22%\n",
      "Epoch [18/100], Loss: 2.4635, Accuracy: 10.90%\n",
      "Test Loss: 2.4502, Accuracy: 10.47%\n",
      "Epoch [19/100], Loss: 2.4622, Accuracy: 10.98%\n",
      "Test Loss: 2.4532, Accuracy: 10.30%\n",
      "Epoch [20/100], Loss: 2.4646, Accuracy: 10.54%\n",
      "Test Loss: 2.4534, Accuracy: 10.44%\n",
      "Epoch [21/100], Loss: 2.4679, Accuracy: 10.49%\n",
      "Test Loss: 2.4512, Accuracy: 10.20%\n",
      "Epoch [22/100], Loss: 2.4634, Accuracy: 10.82%\n",
      "Test Loss: 2.4530, Accuracy: 10.28%\n",
      "Epoch [23/100], Loss: 2.4597, Accuracy: 11.03%\n",
      "Test Loss: 2.4514, Accuracy: 10.41%\n",
      "Epoch [24/100], Loss: 2.4623, Accuracy: 10.81%\n",
      "Test Loss: 2.4507, Accuracy: 10.52%\n",
      "Epoch [25/100], Loss: 2.4615, Accuracy: 10.80%\n",
      "Test Loss: 2.4512, Accuracy: 10.38%\n",
      "Epoch [26/100], Loss: 2.4638, Accuracy: 10.78%\n",
      "Test Loss: 2.4493, Accuracy: 10.44%\n",
      "Epoch [27/100], Loss: 2.4629, Accuracy: 10.73%\n",
      "Test Loss: 2.4507, Accuracy: 10.20%\n",
      "Epoch [28/100], Loss: 2.4624, Accuracy: 10.85%\n",
      "Test Loss: 2.4518, Accuracy: 10.30%\n",
      "Epoch [29/100], Loss: 2.4635, Accuracy: 10.83%\n",
      "Test Loss: 2.4555, Accuracy: 10.60%\n",
      "Epoch [30/100], Loss: 2.4621, Accuracy: 10.73%\n",
      "Test Loss: 2.4523, Accuracy: 10.46%\n",
      "Epoch [31/100], Loss: 2.4603, Accuracy: 10.90%\n",
      "Test Loss: 2.4515, Accuracy: 10.12%\n",
      "Epoch [32/100], Loss: 2.4615, Accuracy: 10.85%\n",
      "Test Loss: 2.4520, Accuracy: 10.32%\n",
      "Epoch [33/100], Loss: 2.4641, Accuracy: 10.92%\n",
      "Test Loss: 2.4547, Accuracy: 10.18%\n",
      "Epoch [34/100], Loss: 2.4655, Accuracy: 10.81%\n",
      "Test Loss: 2.4495, Accuracy: 10.52%\n",
      "Epoch [35/100], Loss: 2.4644, Accuracy: 10.59%\n",
      "Test Loss: 2.4523, Accuracy: 10.36%\n",
      "Epoch [36/100], Loss: 2.4619, Accuracy: 10.90%\n",
      "Test Loss: 2.4507, Accuracy: 10.27%\n",
      "Epoch [37/100], Loss: 2.4644, Accuracy: 10.74%\n",
      "Test Loss: 2.4508, Accuracy: 10.49%\n",
      "Epoch [38/100], Loss: 2.4598, Accuracy: 11.03%\n",
      "Test Loss: 2.4488, Accuracy: 10.38%\n",
      "Epoch [39/100], Loss: 2.4635, Accuracy: 10.81%\n",
      "Test Loss: 2.4521, Accuracy: 10.37%\n",
      "Epoch [40/100], Loss: 2.4606, Accuracy: 10.75%\n",
      "Test Loss: 2.4479, Accuracy: 10.46%\n",
      "Epoch [41/100], Loss: 2.4672, Accuracy: 10.70%\n",
      "Test Loss: 2.4527, Accuracy: 10.30%\n",
      "Epoch [42/100], Loss: 2.4597, Accuracy: 10.86%\n",
      "Test Loss: 2.4504, Accuracy: 10.40%\n",
      "Epoch [43/100], Loss: 2.4643, Accuracy: 10.69%\n",
      "Test Loss: 2.4506, Accuracy: 10.24%\n",
      "Epoch [44/100], Loss: 2.4656, Accuracy: 10.69%\n",
      "Test Loss: 2.4546, Accuracy: 10.24%\n",
      "Epoch [45/100], Loss: 2.4600, Accuracy: 10.91%\n",
      "Test Loss: 2.4492, Accuracy: 10.29%\n",
      "Epoch [46/100], Loss: 2.4598, Accuracy: 10.87%\n",
      "Test Loss: 2.4516, Accuracy: 10.45%\n",
      "Epoch [47/100], Loss: 2.4590, Accuracy: 10.95%\n",
      "Test Loss: 2.4515, Accuracy: 10.49%\n",
      "Epoch [48/100], Loss: 2.4639, Accuracy: 10.81%\n",
      "Test Loss: 2.4520, Accuracy: 10.28%\n",
      "Epoch [49/100], Loss: 2.4611, Accuracy: 10.77%\n",
      "Test Loss: 2.4523, Accuracy: 10.42%\n",
      "Epoch [50/100], Loss: 2.4620, Accuracy: 10.90%\n",
      "Test Loss: 2.4558, Accuracy: 10.35%\n",
      "Epoch [51/100], Loss: 2.4647, Accuracy: 10.77%\n",
      "Test Loss: 2.4523, Accuracy: 10.49%\n",
      "Epoch [52/100], Loss: 2.4614, Accuracy: 10.89%\n",
      "Test Loss: 2.4529, Accuracy: 10.55%\n",
      "Epoch [53/100], Loss: 2.4609, Accuracy: 10.96%\n",
      "Test Loss: 2.4537, Accuracy: 10.27%\n",
      "Epoch [54/100], Loss: 2.4605, Accuracy: 10.97%\n",
      "Test Loss: 2.4511, Accuracy: 10.25%\n",
      "Epoch [55/100], Loss: 2.4632, Accuracy: 10.90%\n",
      "Test Loss: 2.4552, Accuracy: 10.37%\n",
      "Epoch [56/100], Loss: 2.4627, Accuracy: 10.82%\n",
      "Test Loss: 2.4486, Accuracy: 10.38%\n",
      "Epoch [57/100], Loss: 2.4648, Accuracy: 10.73%\n",
      "Test Loss: 2.4488, Accuracy: 10.33%\n",
      "Epoch [58/100], Loss: 2.4633, Accuracy: 10.90%\n",
      "Test Loss: 2.4533, Accuracy: 10.43%\n",
      "Epoch [59/100], Loss: 2.4602, Accuracy: 10.74%\n",
      "Test Loss: 2.4529, Accuracy: 10.15%\n",
      "Epoch [60/100], Loss: 2.4624, Accuracy: 10.91%\n",
      "Test Loss: 2.4522, Accuracy: 10.60%\n",
      "Epoch [61/100], Loss: 2.4605, Accuracy: 10.92%\n",
      "Test Loss: 2.4519, Accuracy: 10.46%\n",
      "Epoch [62/100], Loss: 2.4622, Accuracy: 10.74%\n",
      "Test Loss: 2.4546, Accuracy: 10.35%\n",
      "Epoch [63/100], Loss: 2.4661, Accuracy: 10.65%\n",
      "Test Loss: 2.4491, Accuracy: 10.37%\n",
      "Epoch [64/100], Loss: 2.4613, Accuracy: 10.73%\n",
      "Test Loss: 2.4503, Accuracy: 10.29%\n",
      "Epoch [65/100], Loss: 2.4654, Accuracy: 10.73%\n",
      "Test Loss: 2.4509, Accuracy: 10.24%\n",
      "Epoch [66/100], Loss: 2.4683, Accuracy: 10.69%\n",
      "Test Loss: 2.4505, Accuracy: 10.40%\n",
      "Epoch [67/100], Loss: 2.4620, Accuracy: 10.89%\n",
      "Test Loss: 2.4520, Accuracy: 10.32%\n",
      "Epoch [68/100], Loss: 2.4628, Accuracy: 10.68%\n",
      "Test Loss: 2.4499, Accuracy: 10.38%\n",
      "Epoch [69/100], Loss: 2.4623, Accuracy: 10.77%\n",
      "Test Loss: 2.4519, Accuracy: 10.13%\n",
      "Epoch [70/100], Loss: 2.4597, Accuracy: 10.86%\n",
      "Test Loss: 2.4526, Accuracy: 10.20%\n",
      "Epoch [71/100], Loss: 2.4599, Accuracy: 10.57%\n",
      "Test Loss: 2.4510, Accuracy: 10.37%\n",
      "Epoch [72/100], Loss: 2.4610, Accuracy: 10.87%\n",
      "Test Loss: 2.4503, Accuracy: 10.25%\n",
      "Epoch [73/100], Loss: 2.4636, Accuracy: 10.55%\n",
      "Test Loss: 2.4536, Accuracy: 10.47%\n",
      "Epoch [74/100], Loss: 2.4630, Accuracy: 10.76%\n",
      "Test Loss: 2.4515, Accuracy: 10.41%\n",
      "Epoch [75/100], Loss: 2.4645, Accuracy: 10.78%\n",
      "Test Loss: 2.4501, Accuracy: 10.15%\n",
      "Epoch [76/100], Loss: 2.4598, Accuracy: 10.97%\n",
      "Test Loss: 2.4556, Accuracy: 10.18%\n",
      "Epoch [77/100], Loss: 2.4623, Accuracy: 10.89%\n",
      "Test Loss: 2.4527, Accuracy: 10.21%\n",
      "Epoch [78/100], Loss: 2.4623, Accuracy: 10.76%\n",
      "Test Loss: 2.4529, Accuracy: 10.47%\n",
      "Epoch [79/100], Loss: 2.4601, Accuracy: 10.90%\n",
      "Test Loss: 2.4535, Accuracy: 10.17%\n",
      "Epoch [80/100], Loss: 2.4626, Accuracy: 10.71%\n",
      "Test Loss: 2.4509, Accuracy: 10.32%\n",
      "Epoch [81/100], Loss: 2.4611, Accuracy: 10.69%\n",
      "Test Loss: 2.4534, Accuracy: 10.24%\n",
      "Epoch [82/100], Loss: 2.4624, Accuracy: 10.86%\n",
      "Test Loss: 2.4507, Accuracy: 10.46%\n",
      "Epoch [83/100], Loss: 2.4613, Accuracy: 10.84%\n",
      "Test Loss: 2.4519, Accuracy: 10.32%\n",
      "Epoch [84/100], Loss: 2.4621, Accuracy: 10.76%\n",
      "Test Loss: 2.4493, Accuracy: 10.37%\n",
      "Epoch [85/100], Loss: 2.4651, Accuracy: 10.76%\n",
      "Test Loss: 2.4511, Accuracy: 10.27%\n",
      "Epoch [86/100], Loss: 2.4636, Accuracy: 10.80%\n",
      "Test Loss: 2.4523, Accuracy: 10.37%\n",
      "Epoch [87/100], Loss: 2.4653, Accuracy: 10.83%\n",
      "Test Loss: 2.4498, Accuracy: 10.42%\n",
      "Epoch [88/100], Loss: 2.4608, Accuracy: 10.95%\n",
      "Test Loss: 2.4509, Accuracy: 10.24%\n",
      "Epoch [89/100], Loss: 2.4699, Accuracy: 10.70%\n",
      "Test Loss: 2.4554, Accuracy: 10.48%\n",
      "Epoch [90/100], Loss: 2.4604, Accuracy: 10.81%\n",
      "Test Loss: 2.4531, Accuracy: 10.33%\n",
      "Epoch [91/100], Loss: 2.4647, Accuracy: 10.87%\n",
      "Test Loss: 2.4508, Accuracy: 10.33%\n",
      "Epoch [92/100], Loss: 2.4627, Accuracy: 10.79%\n",
      "Test Loss: 2.4482, Accuracy: 10.45%\n",
      "Epoch [93/100], Loss: 2.4616, Accuracy: 10.83%\n",
      "Test Loss: 2.4533, Accuracy: 10.46%\n",
      "Epoch [94/100], Loss: 2.4617, Accuracy: 10.70%\n",
      "Test Loss: 2.4520, Accuracy: 10.37%\n",
      "Epoch [95/100], Loss: 2.4628, Accuracy: 10.93%\n",
      "Test Loss: 2.4530, Accuracy: 10.28%\n",
      "Epoch [96/100], Loss: 2.4627, Accuracy: 10.81%\n",
      "Test Loss: 2.4542, Accuracy: 10.32%\n",
      "Epoch [97/100], Loss: 2.4614, Accuracy: 10.99%\n",
      "Test Loss: 2.4547, Accuracy: 10.35%\n",
      "Epoch [98/100], Loss: 2.4614, Accuracy: 10.87%\n",
      "Test Loss: 2.4495, Accuracy: 10.44%\n",
      "Epoch [99/100], Loss: 2.4648, Accuracy: 10.71%\n",
      "Test Loss: 2.4501, Accuracy: 10.28%\n",
      "Epoch [100/100], Loss: 2.4631, Accuracy: 10.75%\n",
      "Test Loss: 2.4524, Accuracy: 10.41%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from ResNet20 import ResNet20  # assuming ResNet20 is defined in ResNet20.py\n",
    "from torchvision import transforms\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "# CIFAR-10 dataset\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform_train, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Model\n",
    "model = ResNet20(num_classes=10).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs.clone())  # Clone inputs to avoid inplace operation\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        #loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss/len(train_loader):.4f}, Accuracy: {100.*correct/total:.2f}%')\n",
    "\n",
    "# Testing loop\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Statistics\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    print(f'Test Loss: {test_loss/len(test_loader):.4f}, Accuracy: {100.*correct/total:.2f}%')\n",
    "\n",
    "# Main training and testing loop\n",
    "for epoch in range(num_epochs):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cc6e99-c135-43a8-8582-ccbc9f659112",
   "metadata": {},
   "source": [
    "TEST WITH NEW RESNET20 MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4be585b5-c6a8-439f-8184-d781f0b40b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet20(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc_out): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResNet20(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet20, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Residual blocks\n",
    "        self.layer1 = self._make_layer(in_channels=16, out_channels=16, num_blocks=3, stride=1)\n",
    "        self.layer2 = self._make_layer(in_channels=16, out_channels=32, num_blocks=3, stride=2)\n",
    "        self.layer3 = self._make_layer(in_channels=32, out_channels=64, num_blocks=3, stride=2)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc_out = nn.Linear(64, num_classes)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock(in_channels, out_channels, stride))\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels, stride=1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut(x)\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += shortcut\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = ResNet20()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ec4a8ca-a842-4334-91e7-b344edb86481",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import ResNet20\n",
    "\n",
    "# Define data preprocessing\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# Define data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=4)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "# Instantiate the model\n",
    "resnet20 = ResNet20()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(resnet20.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # Train the model\n",
    "    resnet20.train()\n",
    "    train_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet20(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate the model\n",
    "    resnet20.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = resnet20(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    #print('Epoch [{}/{}], Test Accuracy: {:.2f}%'.format(epoch+1, num_epochs, accuracy))\n",
    "    print(f'Test Loss: {test_loss/len(test_loader):.4f}, Accuracy: {100.*correct/total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081cc40e-1212-4586-bc8a-4b3dfcb29bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet20(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc_out): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/100], Train Loss: 1.6119, Test Loss: 1.3691, Accuracy: 51.46%\n",
      "Epoch [2/100], Train Loss: 1.1025, Test Loss: 1.0943, Accuracy: 62.34%\n",
      "Epoch [3/100], Train Loss: 0.8830, Test Loss: 0.9076, Accuracy: 68.53%\n",
      "Epoch [4/100], Train Loss: 0.7622, Test Loss: 0.8337, Accuracy: 71.87%\n",
      "Epoch [5/100], Train Loss: 0.6960, Test Loss: 0.9722, Accuracy: 68.84%\n",
      "Epoch [6/100], Train Loss: 0.6524, Test Loss: 0.7677, Accuracy: 74.38%\n",
      "Epoch [7/100], Train Loss: 0.6258, Test Loss: 0.8284, Accuracy: 72.07%\n",
      "Epoch [8/100], Train Loss: 0.5992, Test Loss: 0.7076, Accuracy: 76.23%\n",
      "Epoch [9/100], Train Loss: 0.5837, Test Loss: 0.9341, Accuracy: 71.19%\n",
      "Epoch [10/100], Train Loss: 0.5719, Test Loss: 1.0154, Accuracy: 68.15%\n",
      "Epoch [11/100], Train Loss: 0.5541, Test Loss: 0.7721, Accuracy: 74.30%\n",
      "Epoch [12/100], Train Loss: 0.5456, Test Loss: 0.9238, Accuracy: 71.53%\n",
      "Epoch [13/100], Train Loss: 0.5368, Test Loss: 0.9986, Accuracy: 69.60%\n",
      "Epoch [14/100], Train Loss: 0.5229, Test Loss: 0.8773, Accuracy: 72.49%\n",
      "Epoch [15/100], Train Loss: 0.5206, Test Loss: 0.7474, Accuracy: 74.69%\n",
      "Epoch [16/100], Train Loss: 0.5190, Test Loss: 0.8156, Accuracy: 73.03%\n",
      "Epoch [17/100], Train Loss: 0.5045, Test Loss: 0.7561, Accuracy: 76.30%\n",
      "Epoch [18/100], Train Loss: 0.4981, Test Loss: 0.6690, Accuracy: 77.31%\n",
      "Epoch [19/100], Train Loss: 0.5028, Test Loss: 0.6678, Accuracy: 78.01%\n",
      "Epoch [20/100], Train Loss: 0.4880, Test Loss: 0.6730, Accuracy: 77.49%\n",
      "Epoch [21/100], Train Loss: 0.4884, Test Loss: 0.5488, Accuracy: 81.24%\n",
      "Epoch [22/100], Train Loss: 0.4815, Test Loss: 0.7730, Accuracy: 75.66%\n",
      "Epoch [23/100], Train Loss: 0.4828, Test Loss: 0.6952, Accuracy: 75.90%\n",
      "Epoch [24/100], Train Loss: 0.4762, Test Loss: 0.6977, Accuracy: 77.00%\n",
      "Epoch [25/100], Train Loss: 0.4752, Test Loss: 0.6214, Accuracy: 79.67%\n",
      "Epoch [26/100], Train Loss: 0.4650, Test Loss: 0.7763, Accuracy: 73.94%\n",
      "Epoch [27/100], Train Loss: 0.4711, Test Loss: 0.5647, Accuracy: 81.19%\n",
      "Epoch [28/100], Train Loss: 0.4674, Test Loss: 0.6357, Accuracy: 78.99%\n",
      "Epoch [29/100], Train Loss: 0.4641, Test Loss: 0.5644, Accuracy: 80.36%\n",
      "Epoch [30/100], Train Loss: 0.4602, Test Loss: 0.6145, Accuracy: 79.28%\n",
      "Epoch [31/100], Train Loss: 0.4563, Test Loss: 0.6713, Accuracy: 78.19%\n",
      "Epoch [32/100], Train Loss: 0.4548, Test Loss: 1.2811, Accuracy: 63.33%\n",
      "Epoch [33/100], Train Loss: 0.4603, Test Loss: 0.6551, Accuracy: 78.88%\n",
      "Epoch [34/100], Train Loss: 0.4551, Test Loss: 0.6778, Accuracy: 77.84%\n",
      "Epoch [35/100], Train Loss: 0.4546, Test Loss: 0.6242, Accuracy: 78.40%\n",
      "Epoch [36/100], Train Loss: 0.4472, Test Loss: 0.5960, Accuracy: 80.14%\n",
      "Epoch [37/100], Train Loss: 0.4499, Test Loss: 0.6229, Accuracy: 80.46%\n",
      "Epoch [38/100], Train Loss: 0.4552, Test Loss: 0.5904, Accuracy: 80.24%\n",
      "Epoch [39/100], Train Loss: 0.4467, Test Loss: 0.5905, Accuracy: 80.00%\n",
      "Epoch [40/100], Train Loss: 0.4460, Test Loss: 0.6393, Accuracy: 79.73%\n",
      "Epoch [41/100], Train Loss: 0.4427, Test Loss: 0.5497, Accuracy: 81.32%\n",
      "Epoch [42/100], Train Loss: 0.4423, Test Loss: 0.5452, Accuracy: 81.00%\n",
      "Epoch [43/100], Train Loss: 0.4446, Test Loss: 0.5627, Accuracy: 81.12%\n",
      "Epoch [44/100], Train Loss: 0.4415, Test Loss: 0.6919, Accuracy: 77.95%\n",
      "Epoch [45/100], Train Loss: 0.4385, Test Loss: 0.5950, Accuracy: 80.22%\n",
      "Epoch [46/100], Train Loss: 0.4348, Test Loss: 0.5841, Accuracy: 80.85%\n",
      "Epoch [47/100], Train Loss: 0.4367, Test Loss: 0.5577, Accuracy: 82.02%\n",
      "Epoch [48/100], Train Loss: 0.4366, Test Loss: 0.9322, Accuracy: 73.32%\n",
      "Epoch [49/100], Train Loss: 0.4357, Test Loss: 0.6615, Accuracy: 78.58%\n",
      "Epoch [50/100], Train Loss: 0.4365, Test Loss: 0.6775, Accuracy: 77.85%\n",
      "Epoch [51/100], Train Loss: 0.4301, Test Loss: 0.6880, Accuracy: 78.57%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from ResNet20 import ResNet20\n",
    "\n",
    "# Define data preprocessing\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# Define data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=4)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Instantiate the model\n",
    "ResNet20 = ResNet20()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(ResNet20.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # Train the model\n",
    "    ResNet20.train()\n",
    "    train_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = ResNet20(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()  # Accumulate the loss\n",
    "\n",
    "    # Evaluate the model\n",
    "    ResNet20.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = ResNet20(images)\n",
    "            loss = criterion(outputs, labels)  # Compute the loss\n",
    "            test_loss += loss.item()  # Accumulate the loss\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    # Print the progress\n",
    "    #if (epoch+1) % 5 == 0:  # Print every 5 epochs\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d254a111-75e9-497d-9a36-ca4c1e6fa8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet20(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc_out): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 170498071/170498071 [35:59<00:00, 78934.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Epoch [1/100], Train Loss: 1.5910, Test Loss: 1.4299, Accuracy: 49.94%\n",
      "Epoch [2/100], Train Loss: 1.0700, Test Loss: 0.9924, Accuracy: 64.68%\n",
      "Epoch [3/100], Train Loss: 0.8607, Test Loss: 1.2143, Accuracy: 61.98%\n",
      "Epoch [4/100], Train Loss: 0.7593, Test Loss: 1.0093, Accuracy: 68.14%\n",
      "Epoch [5/100], Train Loss: 0.7030, Test Loss: 1.0152, Accuracy: 68.58%\n",
      "Epoch [6/100], Train Loss: 0.6599, Test Loss: 0.7666, Accuracy: 75.05%\n",
      "Epoch [7/100], Train Loss: 0.6352, Test Loss: 1.0176, Accuracy: 66.69%\n",
      "Epoch [8/100], Train Loss: 0.6128, Test Loss: 0.8250, Accuracy: 72.29%\n",
      "Epoch [9/100], Train Loss: 0.5967, Test Loss: 0.9211, Accuracy: 71.29%\n",
      "Epoch [10/100], Train Loss: 0.5812, Test Loss: 1.0391, Accuracy: 67.49%\n",
      "Epoch [11/100], Train Loss: 0.5633, Test Loss: 0.7686, Accuracy: 74.77%\n",
      "Epoch [12/100], Train Loss: 0.5534, Test Loss: 0.6772, Accuracy: 78.12%\n",
      "Epoch [13/100], Train Loss: 0.5527, Test Loss: 0.6534, Accuracy: 78.12%\n",
      "Epoch [14/100], Train Loss: 0.5345, Test Loss: 0.5898, Accuracy: 80.06%\n",
      "Epoch [15/100], Train Loss: 0.5302, Test Loss: 2.1480, Accuracy: 54.67%\n",
      "Epoch [16/100], Train Loss: 0.5187, Test Loss: 0.6663, Accuracy: 77.86%\n",
      "Epoch [17/100], Train Loss: 0.5210, Test Loss: 1.2769, Accuracy: 65.71%\n",
      "Epoch [18/100], Train Loss: 0.5138, Test Loss: 0.6473, Accuracy: 78.32%\n",
      "Epoch [19/100], Train Loss: 0.5098, Test Loss: 1.0208, Accuracy: 68.80%\n",
      "Epoch [20/100], Train Loss: 0.5060, Test Loss: 0.6264, Accuracy: 78.14%\n",
      "Epoch [21/100], Train Loss: 0.5015, Test Loss: 0.5995, Accuracy: 79.90%\n",
      "Epoch [22/100], Train Loss: 0.4963, Test Loss: 0.8686, Accuracy: 73.42%\n",
      "Epoch [23/100], Train Loss: 0.4990, Test Loss: 0.6928, Accuracy: 77.31%\n",
      "Epoch [24/100], Train Loss: 0.4867, Test Loss: 0.6642, Accuracy: 77.67%\n",
      "Epoch [25/100], Train Loss: 0.4883, Test Loss: 0.5352, Accuracy: 81.74%\n",
      "Epoch [26/100], Train Loss: 0.4777, Test Loss: 0.6418, Accuracy: 78.06%\n",
      "Epoch [27/100], Train Loss: 0.4796, Test Loss: 0.7074, Accuracy: 77.33%\n",
      "Epoch [28/100], Train Loss: 0.4821, Test Loss: 0.6093, Accuracy: 79.34%\n",
      "Epoch [29/100], Train Loss: 0.4747, Test Loss: 0.5210, Accuracy: 82.77%\n",
      "Epoch [30/100], Train Loss: 0.4715, Test Loss: 0.5485, Accuracy: 82.09%\n",
      "Epoch [31/100], Train Loss: 0.4756, Test Loss: 0.5832, Accuracy: 80.80%\n",
      "Epoch [32/100], Train Loss: 0.4658, Test Loss: 0.6530, Accuracy: 79.09%\n",
      "Epoch [33/100], Train Loss: 0.4646, Test Loss: 0.7156, Accuracy: 77.31%\n",
      "Epoch [34/100], Train Loss: 0.4708, Test Loss: 0.8949, Accuracy: 72.29%\n",
      "Epoch [35/100], Train Loss: 0.4577, Test Loss: 0.7315, Accuracy: 76.02%\n",
      "Epoch [36/100], Train Loss: 0.4615, Test Loss: 0.6138, Accuracy: 79.82%\n",
      "Epoch [37/100], Train Loss: 0.4567, Test Loss: 0.5478, Accuracy: 81.12%\n",
      "Epoch [38/100], Train Loss: 0.4574, Test Loss: 0.6406, Accuracy: 78.55%\n",
      "Epoch [39/100], Train Loss: 0.4550, Test Loss: 0.6110, Accuracy: 79.65%\n",
      "Epoch [40/100], Train Loss: 0.4598, Test Loss: 0.7015, Accuracy: 76.87%\n",
      "Epoch [41/100], Train Loss: 0.4562, Test Loss: 0.9486, Accuracy: 71.53%\n",
      "Epoch [42/100], Train Loss: 0.4522, Test Loss: 0.7427, Accuracy: 76.09%\n",
      "Epoch [43/100], Train Loss: 0.4513, Test Loss: 0.6408, Accuracy: 78.18%\n",
      "Epoch [44/100], Train Loss: 0.4472, Test Loss: 0.6197, Accuracy: 78.97%\n",
      "Epoch [45/100], Train Loss: 0.4511, Test Loss: 0.7216, Accuracy: 77.48%\n",
      "Epoch [46/100], Train Loss: 0.4475, Test Loss: 0.7571, Accuracy: 76.18%\n",
      "Epoch [47/100], Train Loss: 0.4499, Test Loss: 0.7104, Accuracy: 76.73%\n",
      "Epoch [48/100], Train Loss: 0.4482, Test Loss: 0.7078, Accuracy: 76.25%\n",
      "Epoch [49/100], Train Loss: 0.4399, Test Loss: 0.5543, Accuracy: 80.80%\n",
      "Epoch [50/100], Train Loss: 0.4433, Test Loss: 0.6676, Accuracy: 79.03%\n",
      "Epoch [51/100], Train Loss: 0.4382, Test Loss: 0.5469, Accuracy: 81.70%\n",
      "Epoch [52/100], Train Loss: 0.4372, Test Loss: 0.5768, Accuracy: 80.77%\n",
      "Epoch [53/100], Train Loss: 0.4367, Test Loss: 0.6121, Accuracy: 79.08%\n",
      "Epoch [54/100], Train Loss: 0.4363, Test Loss: 0.6437, Accuracy: 78.64%\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 73\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Train and evaluate the model\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 43\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(model, criterion, optimizer, train_loader, test_loader, num_epochs)\u001b[0m\n\u001b[0;32m     41\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     42\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 43\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:439\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:387\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1040\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1033\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1040\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\multiprocessing\\popen_spawn_win32.py:94\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 94\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     96\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from ResNet20 import ResNet20\n",
    "\n",
    "# Define data preprocessing\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# Define data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=4)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Instantiate the model\n",
    "model = ResNet20()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Training and Evaluation function\n",
    "def train_and_evaluate(model, criterion, optimizer, train_loader, test_loader, num_epochs=100):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train the model\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()  # Accumulate the loss\n",
    "\n",
    "        # Evaluate the model\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)  # Compute the loss\n",
    "                test_loss += loss.item()  # Accumulate the loss\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        test_loss /= len(test_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        # Print the progress\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Train and evaluate the model\n",
    "train_and_evaluate(model, criterion, optimizer, train_loader, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec8528af-eb66-4e73-82eb-f12bcdc5f61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in ResNet20: 272474\n"
     ]
    }
   ],
   "source": [
    "# Initiate the model\n",
    "model = ResNet20()\n",
    "\n",
    "# Count the total number of parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total parameters in ResNet20: {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca7e626-7d42-45ce-a9df-b24b7a1d8b93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/50], Train Loss: 1.5742, Test Loss: 1.4986, Accuracy: 48.73%\n",
      "Epoch [2/50], Train Loss: 1.0804, Test Loss: 1.0815, Accuracy: 62.09%\n",
      "Epoch [3/50], Train Loss: 0.8963, Test Loss: 1.2693, Accuracy: 62.35%\n",
      "Epoch [4/50], Train Loss: 0.7750, Test Loss: 0.9539, Accuracy: 67.24%\n",
      "Epoch [5/50], Train Loss: 0.7102, Test Loss: 0.8339, Accuracy: 72.65%\n",
      "Epoch [6/50], Train Loss: 0.6717, Test Loss: 0.8267, Accuracy: 73.10%\n",
      "Epoch [7/50], Train Loss: 0.6336, Test Loss: 0.8490, Accuracy: 71.96%\n",
      "Epoch [8/50], Train Loss: 0.6067, Test Loss: 0.7637, Accuracy: 74.62%\n",
      "Epoch [9/50], Train Loss: 0.5956, Test Loss: 1.0533, Accuracy: 67.65%\n",
      "Epoch [10/50], Train Loss: 0.5806, Test Loss: 1.1974, Accuracy: 65.27%\n",
      "Epoch [11/50], Train Loss: 0.5670, Test Loss: 0.8245, Accuracy: 74.52%\n",
      "Epoch [12/50], Train Loss: 0.5499, Test Loss: 0.9779, Accuracy: 68.69%\n",
      "Epoch [13/50], Train Loss: 0.5412, Test Loss: 0.7624, Accuracy: 74.62%\n",
      "Epoch [14/50], Train Loss: 0.5288, Test Loss: 0.9171, Accuracy: 72.02%\n",
      "Epoch [15/50], Train Loss: 0.5236, Test Loss: 1.1671, Accuracy: 64.82%\n",
      "Epoch [16/50], Train Loss: 0.5120, Test Loss: 0.7081, Accuracy: 76.93%\n",
      "Epoch [17/50], Train Loss: 0.5099, Test Loss: 0.6353, Accuracy: 79.11%\n",
      "Epoch [18/50], Train Loss: 0.5033, Test Loss: 0.6531, Accuracy: 78.42%\n",
      "Epoch [19/50], Train Loss: 0.5021, Test Loss: 0.6860, Accuracy: 77.69%\n",
      "Epoch [20/50], Train Loss: 0.4959, Test Loss: 1.2466, Accuracy: 67.07%\n",
      "Epoch [21/50], Train Loss: 0.4962, Test Loss: 0.6872, Accuracy: 77.72%\n",
      "Epoch [22/50], Train Loss: 0.4876, Test Loss: 1.1202, Accuracy: 68.32%\n",
      "Epoch [23/50], Train Loss: 0.4848, Test Loss: 0.7760, Accuracy: 75.70%\n",
      "Epoch [24/50], Train Loss: 0.4858, Test Loss: 0.5718, Accuracy: 80.76%\n",
      "Epoch [25/50], Train Loss: 0.4808, Test Loss: 0.8585, Accuracy: 72.64%\n",
      "Epoch [26/50], Train Loss: 0.4716, Test Loss: 0.5942, Accuracy: 79.85%\n",
      "Epoch [27/50], Train Loss: 0.4682, Test Loss: 0.6834, Accuracy: 76.10%\n",
      "Epoch [28/50], Train Loss: 0.4666, Test Loss: 0.5281, Accuracy: 82.01%\n",
      "Epoch [29/50], Train Loss: 0.4672, Test Loss: 0.5845, Accuracy: 80.47%\n",
      "Epoch [30/50], Train Loss: 0.4667, Test Loss: 1.0128, Accuracy: 71.25%\n",
      "Epoch [31/50], Train Loss: 0.4635, Test Loss: 0.5460, Accuracy: 81.49%\n",
      "Epoch [32/50], Train Loss: 0.4610, Test Loss: 0.9239, Accuracy: 71.23%\n",
      "Epoch [33/50], Train Loss: 0.4557, Test Loss: 0.9594, Accuracy: 70.27%\n",
      "Epoch [34/50], Train Loss: 0.4574, Test Loss: 0.7881, Accuracy: 76.65%\n",
      "Epoch [35/50], Train Loss: 0.4558, Test Loss: 0.8581, Accuracy: 70.96%\n",
      "Epoch [36/50], Train Loss: 0.4529, Test Loss: 0.5526, Accuracy: 81.95%\n",
      "Epoch [37/50], Train Loss: 0.4536, Test Loss: 0.6506, Accuracy: 79.30%\n",
      "Epoch [38/50], Train Loss: 0.4533, Test Loss: 0.9425, Accuracy: 73.11%\n",
      "Epoch [39/50], Train Loss: 0.4449, Test Loss: 0.7173, Accuracy: 77.49%\n",
      "Epoch [40/50], Train Loss: 0.4481, Test Loss: 0.6484, Accuracy: 79.51%\n",
      "Epoch [41/50], Train Loss: 0.4456, Test Loss: 0.6080, Accuracy: 79.00%\n",
      "Epoch [42/50], Train Loss: 0.4445, Test Loss: 0.8600, Accuracy: 74.65%\n",
      "Epoch [43/50], Train Loss: 0.4395, Test Loss: 0.5846, Accuracy: 80.61%\n",
      "Epoch [44/50], Train Loss: 0.4428, Test Loss: 0.6845, Accuracy: 77.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from ResNet20 import ResNet20\n",
    "\n",
    "# Define data preprocessing\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# Define data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=4)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Instantiate the model\n",
    "model = ResNet20()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Training and Evaluation function\n",
    "def train_and_evaluate(model, criterion, optimizer, train_loader, test_loader, num_epochs=50):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train the model\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()  # Accumulate the loss\n",
    "\n",
    "        # Evaluate the model\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)  # Compute the loss\n",
    "                test_loss += loss.item()  # Accumulate the loss\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        test_loss /= len(test_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        # Print the progress\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Train and evaluate the model\n",
    "train_and_evaluate(model, criterion, optimizer, train_loader, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3fa4cd-501d-4103-8e67-3b66c072697f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet20(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc_out): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from ResNet20 import ResNet20\n",
    "\n",
    "# Define data preprocessing\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# Define data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=4)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Instantiate the model\n",
    "model = ResNet20()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Training and Evaluation function\n",
    "def train_and_evaluate(model, criterion, optimizer, train_loader, test_loader, num_epochs=50):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train the model\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()  # Accumulate the loss\n",
    "\n",
    "        # Evaluate the model\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)  # Compute the loss\n",
    "                test_loss += loss.item()  # Accumulate the loss\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        test_loss /= len(test_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        # Print the progress\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Train and evaluate the model\n",
    "train_and_evaluate(model, criterion, optimizer, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369586f5-18cc-4cf4-897b-8a8ec292314d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
